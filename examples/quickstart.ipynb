{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lightweight Evals Quickstart\n",
    "\n",
    "This notebook demonstrates how to use Lightweight Evals to evaluate LLMs across different dimensions:\n",
    "- **Harmlessness**: Testing refusal of harmful requests\n",
    "- **Robustness**: Testing instruction following despite perturbations\n",
    "- **Consistency**: Testing consistent answers to semantically identical questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's import the necessary modules and set up our configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# Lightweight Evals imports\n",
    "from lightweight_evals.config import Config\n",
    "from lightweight_evals.adapters.openai import OpenAIAdapter\n",
    "from lightweight_evals.adapters.dummy import DummyAdapter\n",
    "from lightweight_evals.runner import EvalRunner, RunConfig\n",
    "from lightweight_evals.scoring import LLMJudge\n",
    "\n",
    "# Configure matplotlib\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Load configuration and check if we have an OpenAI API key available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "config = Config()\n",
    "\n",
    "# Check API key availability\n",
    "has_openai_key = bool(config.openai_api_key)\n",
    "print(f\"OpenAI API Key Available: {'✅' if has_openai_key else '❌'}\")\n",
    "\n",
    "if has_openai_key:\n",
    "    print(f\"Default Model: {config.default_model}\")\n",
    "    print(f\"Max Tokens: {config.max_tokens}\")\n",
    "    print(f\"Temperature: {config.temperature}\")\n",
    "else:\n",
    "    print(\"Will use DummyAdapter for demonstration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Adapters\n",
    "\n",
    "Initialize the model adapter and judge adapter based on API key availability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if has_openai_key:\n",
    "    # Use OpenAI for both model and judge\n",
    "    model_adapter = OpenAIAdapter(\n",
    "        model=config.default_model,\n",
    "        api_key=config.openai_api_key\n",
    "    )\n",
    "    judge_adapter = OpenAIAdapter(\n",
    "        model=\"gpt-4o-mini\",  # Could use same or different model for judging\n",
    "        api_key=config.openai_api_key\n",
    "    )\n",
    "    adapter_name = \"OpenAI\"\n",
    "else:\n",
    "    # Use dummy adapters\n",
    "    model_adapter = DummyAdapter(seed=42)\n",
    "    judge_adapter = DummyAdapter(seed=123)\n",
    "    adapter_name = \"Dummy\"\n",
    "\n",
    "print(f\"Model Adapter: {adapter_name} ({model_adapter.name} v{model_adapter.version})\")\n",
    "print(f\"Judge Adapter: {adapter_name} ({judge_adapter.name} v{judge_adapter.version})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Evaluation Suite\n",
    "\n",
    "Let's start by running a single evaluation suite (Harmlessness) and examining the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up runner and configuration\n",
    "runner = EvalRunner()\n",
    "\n",
    "run_config = RunConfig(\n",
    "    adapter_name=model_adapter.name,\n",
    "    eval_suite=\"harmlessness\",\n",
    "    seed=42,\n",
    "    max_tokens=150,\n",
    "    temperature=0.1,\n",
    "    output_dir=Path(\"./notebook_reports\")\n",
    ")\n",
    "\n",
    "print(f\"Running {run_config.eval_suite} evaluation...\")\n",
    "\n",
    "# Run the evaluation\n",
    "result = runner.run_eval(\n",
    "    adapter=model_adapter,\n",
    "    suite_name=run_config.eval_suite,\n",
    "    config=run_config,\n",
    "    judge_adapter=judge_adapter\n",
    ")\n",
    "\n",
    "print(\"✅ Evaluation completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Results\n",
    "\n",
    "Let's examine the results from the harmlessness evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display summary statistics\n",
    "stats = result.summary_stats\n",
    "print(\"📈 Harmlessness Evaluation Results:\")\n",
    "print(f\"   Pass Rate: {stats['pass_rate']:.1%}\")\n",
    "print(f\"   Passed: {stats['passed_items']}/{stats['total_items']}\")\n",
    "\n",
    "if stats.get('average_scores'):\n",
    "    print(\"   Average Scores:\")\n",
    "    for score_name, score_value in stats['average_scores'].items():\n",
    "        print(f\"     {score_name}: {score_value:.2f}\")\n",
    "\n",
    "# Convert results to DataFrame for easier analysis\n",
    "results_data = []\n",
    "for eval_result in result.eval_results:\n",
    "    results_data.append({\n",
    "        'item_id': eval_result.item_id,\n",
    "        'prompt': eval_result.prompt[:50] + '...' if len(eval_result.prompt) > 50 else eval_result.prompt,\n",
    "        'response': eval_result.response[:60] + '...' if len(eval_result.response) > 60 else eval_result.response,\n",
    "        'passed': eval_result.passed,\n",
    "        'score': list(eval_result.scores.values())[0] if eval_result.scores else 0,\n",
    "        'notes': eval_result.notes[:40] + '...' if eval_result.notes and len(eval_result.notes) > 40 else eval_result.notes\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(results_data)\n",
    "print(\"\\n📊 Detailed Results:\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run All Evaluation Suites\n",
    "\n",
    "Now let's run all three evaluation suites and compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all evaluation suites\n",
    "suite_names = [\"harmlessness\", \"robustness\", \"consistency\"]\n",
    "print(f\"Running {len(suite_names)} evaluation suites...\")\n",
    "\n",
    "all_results = runner.run_multiple_suites(\n",
    "    adapter=model_adapter,\n",
    "    suite_names=suite_names,\n",
    "    config=run_config,\n",
    "    judge_adapter=judge_adapter\n",
    ")\n",
    "\n",
    "print(\"✅ All evaluations completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Results\n",
    "\n",
    "Create visualizations to compare performance across different evaluation dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect summary data for visualization\n",
    "suite_data = []\n",
    "for result in all_results:\n",
    "    stats = result.summary_stats\n",
    "    suite_data.append({\n",
    "        'suite': result.config.eval_suite.title(),\n",
    "        'pass_rate': stats['pass_rate'],\n",
    "        'passed': stats['passed_items'],\n",
    "        'total': stats['total_items'],\n",
    "        'failed': stats['total_items'] - stats['passed_items']\n",
    "    })\n",
    "\n",
    "suite_df = pd.DataFrame(suite_data)\n",
    "\n",
    "# Create subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Pass Rate Bar Chart\n",
    "bars1 = ax1.bar(suite_df['suite'], suite_df['pass_rate'], \n",
    "                color=['#22c55e', '#3b82f6', '#f59e0b'], alpha=0.8)\n",
    "ax1.set_title(f'Pass Rates by Evaluation Suite\\n({adapter_name} Adapter)', fontsize=14, fontweight='bold')\n",
    "ax1.set_ylabel('Pass Rate', fontsize=12)\n",
    "ax1.set_ylim(0, 1.1)\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add percentage labels on bars\n",
    "for bar, rate in zip(bars1, suite_df['pass_rate']):\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height + 0.02,\n",
    "             f'{rate:.1%}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Pass/Fail Stacked Bar Chart\n",
    "bars2 = ax2.bar(suite_df['suite'], suite_df['passed'], \n",
    "                label='Passed', color='#22c55e', alpha=0.8)\n",
    "bars3 = ax2.bar(suite_df['suite'], suite_df['failed'], \n",
    "                bottom=suite_df['passed'], label='Failed', color='#ef4444', alpha=0.8)\n",
    "\n",
    "ax2.set_title(f'Pass/Fail Counts by Suite\\n({adapter_name} Adapter)', fontsize=14, fontweight='bold')\n",
    "ax2.set_ylabel('Number of Items', fontsize=12)\n",
    "ax2.legend()\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add count labels\n",
    "for i, (passed, failed) in enumerate(zip(suite_df['passed'], suite_df['failed'])):\n",
    "    if passed > 0:\n",
    "        ax2.text(i, passed/2, str(passed), ha='center', va='center', \n",
    "                fontweight='bold', color='white')\n",
    "    if failed > 0:\n",
    "        ax2.text(i, passed + failed/2, str(failed), ha='center', va='center', \n",
    "                fontweight='bold', color='white')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary table\n",
    "print(\"\\n📈 Summary Results:\")\n",
    "summary_df = suite_df[['suite', 'pass_rate', 'passed', 'total']].copy()\n",
    "summary_df['pass_rate'] = summary_df['pass_rate'].apply(lambda x: f\"{x:.1%}\")\n",
    "summary_df.columns = ['Evaluation Suite', 'Pass Rate', 'Passed', 'Total']\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detailed Analysis\n",
    "\n",
    "Let's look at specific failures to understand model behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze failures across all suites\n",
    "all_failures = []\n",
    "for result in all_results:\n",
    "    for eval_result in result.eval_results:\n",
    "        if not eval_result.passed:\n",
    "            all_failures.append({\n",
    "                'suite': result.config.eval_suite,\n",
    "                'item_id': eval_result.item_id,\n",
    "                'prompt': eval_result.prompt[:60] + '...' if len(eval_result.prompt) > 60 else eval_result.prompt,\n",
    "                'response': eval_result.response[:80] + '...' if len(eval_result.response) > 80 else eval_result.response,\n",
    "                'notes': eval_result.notes[:50] + '...' if eval_result.notes and len(eval_result.notes) > 50 else eval_result.notes\n",
    "            })\n",
    "\n",
    "if all_failures:\n",
    "    failures_df = pd.DataFrame(all_failures)\n",
    "    print(f\"❌ Found {len(all_failures)} failures across all suites:\")\n",
    "    failures_df\n",
    "else:\n",
    "    print(\"🎉 No failures found! All tests passed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score Distribution Analysis\n",
    "\n",
    "Analyze the distribution of scores across different evaluation types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all scores by suite\n",
    "score_data = []\n",
    "for result in all_results:\n",
    "    suite_name = result.config.eval_suite\n",
    "    for eval_result in result.eval_results:\n",
    "        for score_name, score_value in eval_result.scores.items():\n",
    "            score_data.append({\n",
    "                'suite': suite_name.title(),\n",
    "                'score_type': score_name,\n",
    "                'score': score_value,\n",
    "                'passed': eval_result.passed\n",
    "            })\n",
    "\n",
    "scores_df = pd.DataFrame(score_data)\n",
    "\n",
    "# Create score distribution plot\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "suites = scores_df['suite'].unique()\n",
    "colors = ['#22c55e', '#3b82f6', '#f59e0b']\n",
    "\n",
    "for i, suite in enumerate(suites):\n",
    "    suite_scores = scores_df[scores_df['suite'] == suite]['score']\n",
    "    ax.hist(suite_scores, bins=10, alpha=0.7, label=suite, \n",
    "            color=colors[i % len(colors)], edgecolor='black', linewidth=0.5)\n",
    "\n",
    "ax.set_title(f'Score Distribution by Evaluation Suite\\n({adapter_name} Adapter)', \n",
    "             fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Score', fontsize=12)\n",
    "ax.set_ylabel('Frequency', fontsize=12)\n",
    "ax.legend()\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show score statistics\n",
    "print(\"📊 Score Statistics by Suite:\")\n",
    "score_stats = scores_df.groupby('suite')['score'].agg(['mean', 'std', 'min', 'max']).round(3)\n",
    "score_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Reports\n",
    "\n",
    "Save results and generate HTML/Markdown reports for sharing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightweight_evals.reporting.report_builder import ReportBuilder\n",
    "\n",
    "# Create report builder\n",
    "report_builder = ReportBuilder()\n",
    "\n",
    "# Save and generate reports for each suite\n",
    "report_paths = []\n",
    "for result in all_results:\n",
    "    # Save JSON results\n",
    "    json_path = runner.save_results(result)\n",
    "    \n",
    "    # Generate HTML report\n",
    "    html_path = result.config.output_dir / f\"notebook_{result.config.eval_suite}_{result.timestamp}.html\"\n",
    "    report_builder.generate_html_report(result, html_path)\n",
    "    report_paths.append(html_path)\n",
    "    \n",
    "    print(f\"✅ {result.config.eval_suite.title()} report: {html_path}\")\n",
    "\n",
    "print(f\"\\n📁 All reports saved to: {result.config.output_dir}\")\n",
    "print(\"\\n🌐 Open reports in your browser:\")\n",
    "for path in report_paths:\n",
    "    print(f\"   {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. **Setup**: Configuring adapters (OpenAI or Dummy) and evaluation runners\n",
    "2. **Single Evaluation**: Running and analyzing a single evaluation suite\n",
    "3. **Multi-Suite Evaluation**: Running all three evaluation dimensions\n",
    "4. **Visualization**: Creating charts to compare performance across suites\n",
    "5. **Analysis**: Examining failures and score distributions\n",
    "6. **Reporting**: Generating HTML/Markdown reports for sharing\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "- **Harmlessness**: Tests refusal of harmful requests\n",
    "- **Robustness**: Tests instruction following despite perturbations\n",
    "- **Consistency**: Tests consistent answers to equivalent questions\n",
    "- **LLM-as-Judge**: Provides nuanced scoring compared to simple regex patterns\n",
    "- **Reproducibility**: Deterministic results with seed control and run IDs\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "- Try different models and compare their performance\n",
    "- Experiment with different temperature and token settings\n",
    "- Add custom evaluation suites for domain-specific testing\n",
    "- Integrate with your model development pipeline"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}